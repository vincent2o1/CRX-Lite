{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO00fSZx6gzu1h9SYHm0qYt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent2o1/CRX-Lite/blob/main/CRX_Lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Focusing agent\n",
        "A function that process piece by piece information about the input source and send it to the ring tree."
      ],
      "metadata": {
        "id": "2lfTYlrHKCTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zA6UshrWLbTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ElZSJvlJ3eu"
      },
      "outputs": [],
      "source": [
        "#FOCUSING AGENT\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zlib\n",
        "import base64\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "# Global storage for agent movements and spot IDs\n",
        "agent_movements = {}\n",
        "spot_ids = {}\n",
        "\n",
        "def image_to_pixels(image_path):\n",
        "    \"\"\"\n",
        "    Convert an image to grayscale pixel values and resize it to 64x64.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        raise ValueError(\"Image not found or invalid format.\")\n",
        "\n",
        "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "    return image\n",
        "\n",
        "def visualize_agent_movements(image, movements, title=\"Agent Movements\"):\n",
        "    resized_image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(resized_image, cmap='gray')\n",
        "\n",
        "    y_vals, x_vals = zip(*movements)\n",
        "    plt.plot(x_vals, y_vals, marker='o', color='red', linestyle='-')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def visualize_spots(image, movements, focus_size=(16, 16)):\n",
        "    focus_h, focus_w = focus_size\n",
        "    num_spots = len(movements)\n",
        "    cols = min(4, num_spots)\n",
        "    rows = (num_spots + cols - 1) // cols  # Calculate rows to fit all spots\n",
        "\n",
        "    plt.figure(figsize=(cols * 4, rows * 4))\n",
        "\n",
        "    for i, (y, x) in enumerate(movements):\n",
        "        spot = image[y:y+focus_h, x:x+focus_w]\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(spot, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Spot {i+1}\")\n",
        "\n",
        "    plt.suptitle(\"Focused Spots\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def list_to_reversible_id(pixel_list):\n",
        "    array = np.array(pixel_list, dtype=np.uint8)\n",
        "    compressed = zlib.compress(array.tobytes())\n",
        "    encoded = base64.urlsafe_b64encode(compressed).decode()\n",
        "    return encoded\n",
        "\n",
        "def id_to_list(image_id, original_length):\n",
        "    compressed = base64.urlsafe_b64decode(image_id)\n",
        "    image_bytes = zlib.decompress(compressed)\n",
        "    pixel_list = np.frombuffer(image_bytes, dtype=np.uint8).tolist()\n",
        "    return pixel_list[:original_length]\n",
        "\n",
        "def calculate_variance(region):\n",
        "    return np.var(region)\n",
        "\n",
        "def is_unique_spot(y, x, image, focus_h, focus_w):\n",
        "    spot = image[y:y+focus_h, x:x+focus_w].flatten().tolist()\n",
        "    spot_id = list_to_reversible_id(spot)\n",
        "    if spot_id in spot_ids:\n",
        "        return False\n",
        "    spot_ids[spot_id] = (y, x)\n",
        "   # save_spot_ids_to_drive(spot_id)\n",
        "    return True\n",
        "\n",
        "def generate_importance_map(image, threshold_var=50, threshold_edge=30):\n",
        "    \"\"\"\n",
        "    Generate an importance map across the entire image based on variance and edge detection.\n",
        "\n",
        "    Args:\n",
        "        image: Input grayscale image\n",
        "        threshold_var: Variance threshold for important regions\n",
        "        threshold_edge: Edge detection threshold\n",
        "\n",
        "    Returns:\n",
        "        Binary importance map where 1 indicates important regions\n",
        "    \"\"\"\n",
        "    # Initialize importance map\n",
        "    importance_map = np.zeros_like(image, dtype=np.uint8)\n",
        "\n",
        "    # Detect edges using Canny edge detection\n",
        "    edges = cv2.Canny(image, threshold_edge, threshold_edge * 3)\n",
        "\n",
        "    # Calculate local variance across the entire image using a sliding window\n",
        "    variance_map = np.zeros_like(image, dtype=np.float32)\n",
        "    kernel_size = 5  # Size for local variance calculation\n",
        "\n",
        "    # Pad image for boundary handling\n",
        "    padded_image = cv2.copyMakeBorder(image, kernel_size//2, kernel_size//2,\n",
        "                                      kernel_size//2, kernel_size//2,\n",
        "                                      cv2.BORDER_REFLECT)\n",
        "\n",
        "    # Calculate variance for each pixel position\n",
        "    for y in range(image.shape[0]):\n",
        "        for x in range(image.shape[1]):\n",
        "            window = padded_image[y:y+kernel_size, x:x+kernel_size]\n",
        "            variance_map[y, x] = np.var(window)\n",
        "\n",
        "    # Mark pixels as important if they have high variance or are part of edges\n",
        "    importance_map = np.logical_or(variance_map > threshold_var, edges > 0).astype(np.uint8)\n",
        "\n",
        "    # Optional: Apply morphological operations to clean up the map\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    importance_map = cv2.dilate(importance_map, kernel, iterations=1)\n",
        "\n",
        "    return importance_map\n",
        "\n",
        "def focusing_agent(image, image1, importance_map, dual_process=False,\n",
        "                  request_input=None, training=False, focus_size=(16, 16),\n",
        "                  similarity_threshold=0.8, testing=False):\n",
        "    \"\"\"\n",
        "    Focusing agent that returns stored spots that match with the input image.\n",
        "    Uses SSIM matching, rotation, and limited zoom in/out.\n",
        "    \"\"\"\n",
        "    height, width = image.shape\n",
        "    focus_h, focus_w = focus_size\n",
        "    global spot_ids\n",
        "    spot_ids = retrieve_all_spot_ids_from_drive_all()\n",
        "\n",
        "    # Testing mode - extract all familiar spots with selective matching\n",
        "    if testing:\n",
        "        focused_regions = []   # Will store the matched stored spots\n",
        "        focused_regions1 = []  # Will store the matched stored spots (secondary image)\n",
        "        movements = []         # Will store the positions where matches were found\n",
        "        matched_positions = set()  # Track positions we've already checked to avoid duplicates\n",
        "\n",
        "        # Convert all stored spot IDs to images once for efficiency\n",
        "        stored_spots = {}\n",
        "        for stored_id in spot_ids.keys():\n",
        "            try:\n",
        "                stored_spot_pixels = id_to_list(stored_id, focus_h * focus_w)\n",
        "                stored_spots[stored_id] = np.array(stored_spot_pixels, dtype=np.uint8).reshape(focus_h, focus_w)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading stored spot {stored_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # For each position in the image\n",
        "        step_size = 1  # Reduced from 2 to ensure we don't miss matches\n",
        "        for y in range(0, height - focus_h + 1, step_size):\n",
        "            for x in range(0, width - focus_w + 1, step_size):\n",
        "                # Create position key for tracking\n",
        "                pos_key = (y, x)\n",
        "                if pos_key in matched_positions:\n",
        "                    continue\n",
        "\n",
        "                # Skip if not an important region\n",
        "                region_importance = importance_map[y:y+focus_h, x:x+focus_w]\n",
        "                if not np.any(region_importance > 0):\n",
        "                    continue\n",
        "\n",
        "                # Extract the current spot\n",
        "                current_spot = image[y:y+focus_h, x:x+focus_w]\n",
        "                best_match = None\n",
        "                best_similarity = 0\n",
        "\n",
        "                # Check against all stored spots\n",
        "                for stored_id, stored_spot in stored_spots.items():\n",
        "                    try:\n",
        "                        # 1. Basic SSIM matching\n",
        "                        basic_similarity = ssim(current_spot, stored_spot, data_range=255)\n",
        "                        similarity = basic_similarity\n",
        "\n",
        "                        if similarity <= similarity_threshold:\n",
        "                            # 2. Rotation matching - try different angles\n",
        "                            for angle in [90, 180, 270]:\n",
        "                                rotated_spot = np.rot90(current_spot, k=angle//90)\n",
        "                                rot_similarity = ssim(rotated_spot, stored_spot, data_range=255)\n",
        "                                if rot_similarity > similarity:\n",
        "                                    similarity = rot_similarity\n",
        "\n",
        "                            # 3. Limited zoom in/out if still no match\n",
        "                            if similarity <= similarity_threshold:\n",
        "                                for scale_factor in [0.95, 1.05]:\n",
        "                                    # Scale the current spot\n",
        "                                    scaled_size = int(focus_h * scale_factor)\n",
        "                                    if scaled_size <= 0 or scaled_size >= height:\n",
        "                                        continue\n",
        "\n",
        "                                    scaled_spot = cv2.resize(current_spot, (scaled_size, scaled_size))\n",
        "\n",
        "                                    # Crop or pad to match original size\n",
        "                                    if scale_factor > 1:  # Larger than original - crop center\n",
        "                                        start = (scaled_size - focus_h) // 2\n",
        "                                        scaled_spot = scaled_spot[start:start+focus_h, start:start+focus_h]\n",
        "                                    else:  # Smaller than original - pad with zeros\n",
        "                                        pad_size = (focus_h - scaled_size) // 2\n",
        "                                        scaled_spot = np.pad(scaled_spot, ((pad_size, pad_size), (pad_size, pad_size)),\n",
        "                                                           'constant', constant_values=0)\n",
        "\n",
        "                                        # Ensure exact dimensions\n",
        "                                        scaled_spot = cv2.resize(scaled_spot, (focus_h, focus_w))\n",
        "\n",
        "                                    scale_similarity = ssim(scaled_spot, stored_spot, data_range=255)\n",
        "                                    if scale_similarity > similarity:\n",
        "                                        similarity = scale_similarity\n",
        "\n",
        "                        # Track the best match for this position\n",
        "                        if similarity > similarity_threshold and similarity > best_similarity:\n",
        "                            best_similarity = similarity\n",
        "                            best_match = stored_id\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in spot matching: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # If we found a match at this position\n",
        "                if best_match:\n",
        "                    # Add the stored spot to the results\n",
        "                    matched_spot = stored_spots[best_match]\n",
        "                    focused_regions.append(matched_spot)\n",
        "                    focused_regions1.append(matched_spot)\n",
        "                    movements.append((y, x))\n",
        "\n",
        "                    # Mark this position as matched\n",
        "                    matched_positions.add(pos_key)\n",
        "\n",
        "                    # Also mark nearby positions to avoid redundant matches (optional)\n",
        "                    buffer = 2  # Adjust based on how close matches should be allowed\n",
        "                    for by in range(max(0, y-buffer), min(height-focus_h+1, y+buffer+1)):\n",
        "                        for bx in range(max(0, x-buffer), min(width-focus_w+1, x+buffer+1)):\n",
        "                            matched_positions.add((by, bx))\n",
        "\n",
        "        if not movements:\n",
        "            print(\"No familiar spots found in the image\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Found {len(movements)} familiar spots in the image\")\n",
        "        visualize_agent_movements(image, movements)\n",
        "        visualize_spots(image, movements)\n",
        "\n",
        "        # Return the stored spots that matched with the input image\n",
        "        return focused_regions, focused_regions1\n",
        "\n",
        "    # Storage for already processed spots to ensure uniqueness\n",
        "    unique_spots = []\n",
        "    unique_spot_hashes = set()\n",
        "\n",
        "    if training:\n",
        "        focused_regions = []\n",
        "        movements = []\n",
        "        checked_positions = set()\n",
        "\n",
        "        # Find all possible spot positions\n",
        "        possible_positions = []\n",
        "        for y in range(0, height - focus_h + 1):\n",
        "            for x in range(0, width - focus_w + 1):\n",
        "                # Consider this position only if it contains important pixels\n",
        "                region_importance = importance_map[y:y+focus_h, x:x+focus_w]\n",
        "                if np.any(region_importance > 0):\n",
        "                    possible_positions.append((y, x))\n",
        "\n",
        "        # Sort positions by importance (number of important pixels in region)\n",
        "        sorted_positions = sorted(\n",
        "            possible_positions,\n",
        "            key=lambda pos: np.sum(importance_map[pos[0]:pos[0]+focus_h, pos[1]:pos[1]+focus_w]),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Process positions in order of importance\n",
        "        for y, x in sorted_positions:\n",
        "            if (y, x) in checked_positions:\n",
        "                continue\n",
        "\n",
        "            region = image[y:y+focus_h, x:x+focus_w]\n",
        "\n",
        "            # Skip if variance is too low\n",
        "            if calculate_variance(region) <= 50:\n",
        "                checked_positions.add((y, x))\n",
        "                continue\n",
        "\n",
        "            # Check if this spot is unique compared to all previously found spots\n",
        "            spot_is_unique = True\n",
        "            region_flat = region.flatten()\n",
        "            spot_hash = hash(region_flat.tobytes())\n",
        "\n",
        "            # First check simple hash for quick elimination\n",
        "            if spot_hash in unique_spot_hashes:\n",
        "                spot_is_unique = False\n",
        "            else:\n",
        "                # Also check using structural similarity to catch visually similar spots\n",
        "                for prev_region, _ in unique_spots:\n",
        "                    similarity = ssim(region, prev_region, data_range=255)\n",
        "                    if similarity > similarity_threshold:\n",
        "                        spot_is_unique = False\n",
        "                        break\n",
        "\n",
        "            if spot_is_unique:\n",
        "                unique_spot_hashes.add(spot_hash)\n",
        "                unique_spots.append((region, (y, x)))\n",
        "                movements.append((y, x))\n",
        "\n",
        "                # Create spot ID for the global dictionary\n",
        "                spot_id = list_to_reversible_id(region.flatten().tolist())\n",
        "                spot_ids[spot_id] = (y, x)\n",
        "\n",
        "                # Mark surrounding positions as checked to avoid nearly duplicate spots\n",
        "                for dy in range(-focus_h//2, focus_h//2 + 1, focus_h//4):\n",
        "                    for dx in range(-focus_w//2, focus_w//2 + 1, focus_w//4):\n",
        "                        ny, nx = y + dy, x + dx\n",
        "                        if (0 <= ny <= height - focus_h and\n",
        "                            0 <= nx <= width - focus_w):\n",
        "                            checked_positions.add((ny, nx))\n",
        "            else:\n",
        "                checked_positions.add((y, x))\n",
        "\n",
        "        if not movements:\n",
        "            print(\"Input fully processed. No new unique important portions found.\")\n",
        "            return None\n",
        "\n",
        "        visualize_agent_movements(image, movements)\n",
        "        visualize_spots(image, movements)\n",
        "\n",
        "        focused_regions = [image[y:y+focus_h, x:x+focus_w] for y, x in movements]\n",
        "        focused_regions1 = [image1[y:y+focus_h, x:x+focus_w] for y, x in movements]\n",
        "\n",
        "        return focused_regions, focused_regions1\n",
        "    # Existing logic for dual_process remains unchanged\n",
        "    if dual_process and request_input is not None:\n",
        "        match_found = False\n",
        "        requested_position = None\n",
        "\n",
        "       # print(\"0\")\n",
        "        request_input = id_to_list(request_input, 1024)\n",
        "        # Ensure request_input is a NumPy array\n",
        "        if isinstance(request_input, list):\n",
        "            request_input = np.array(request_input)\n",
        "        # Ensure request_input is in a 2D format (assuming it's meant to be 16x16)\n",
        "        side_length = int(np.sqrt(len(request_input)))\n",
        "\n",
        "        if side_length * side_length != len(request_input):\n",
        "            print(\"Error: request_input cannot form a square image.\")\n",
        "            return \"Error: Invalid input dimensions.\"\n",
        "\n",
        "        request_input_2d = request_input.reshape((side_length, side_length))\n",
        "\n",
        "        # Resize to the target dimensions\n",
        "        request_input_resized = cv2.resize(request_input_2d, (focus_w, focus_h))\n",
        "       # print(request_input_resized)\n",
        "\n",
        "        #print(request_input_resized)\n",
        "        for y in range(height - focus_h + 1):\n",
        "            #print(\"1\")\n",
        "            for x in range(width - focus_w + 1):\n",
        "                #print(\"2\")\n",
        "                region = image[y:y+focus_h, x:x+focus_w]\n",
        "                similarity = ssim(region.astype(np.float32), request_input_resized.astype(np.float32), data_range=255)\n",
        "                if similarity > 0.40:\n",
        "                    print(\"Match found in image source\")\n",
        "                    requested_position = (y, x)\n",
        "                    match_found = True\n",
        "                    break\n",
        "            if match_found:\n",
        "                break\n",
        "       # print(f\"match found - {match_found}\")\n",
        "        if not match_found:\n",
        "            return match_found\n",
        "\n",
        "        request_input_id = list_to_reversible_id(request_input.flatten().tolist())\n",
        "       # print(\"2\")\n",
        "       # print(f\"spot - {spot_ids}\")\n",
        "        # Convert spot_ids to a list of tuples (key, value)\n",
        "        spot_id_items = list(spot_ids.items())\n",
        "\n",
        "        if request_input_id in spot_ids:\n",
        "\n",
        "            print(\"Matching ID found\")\n",
        "            current_position = spot_ids[request_input_id]\n",
        "            print(f\"Current position: {current_position}\")\n",
        "\n",
        "            current_index = list(spot_ids.values()).index(current_position)\n",
        "            #make sure this next id is present in input source.\n",
        "            if current_index + 1 < len(spot_ids):\n",
        "                #print(\"3\")\n",
        "                next_position = list(spot_ids.values())[current_index + 1]\n",
        "                next_spot_id_key, next_position_ = spot_id_items[current_index + 1]\n",
        "                print(\"Connected spot found\")\n",
        "               # spot_image = image[next_position[0]:next_position[0]+focus_h,\n",
        "                                    #next_position[1]:next_position[1]+focus_w]\n",
        "                #plt.figure()\n",
        "                #plt.imshow(spot_image, cmap='gray')\n",
        "               # plt.title(\"Connected Spot Visualization\")\n",
        "                #plt.show()\n",
        "                return (next_spot_id_key, match_found)\n",
        "\n",
        "def save_spot_ids_to_drive(spot_ids):\n",
        "    \"\"\"Save spot IDs with a counter for uniqueness.\"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids'\n",
        "    os.makedirs(drive_folder, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    # Find existing files and determine the next counter\n",
        "    existing_files = [f for f in os.listdir(drive_folder) if f.startswith('spot_ids_') and f.endswith('.json')]\n",
        "    next_counter = len(existing_files) + 1\n",
        "\n",
        "    # Create a filename using a counter\n",
        "    file_path = os.path.join(drive_folder, f'spot_ids_{next_counter:04d}.json')\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(spot_ids, f, indent=4)\n",
        "        print(f\"Spot IDs successfully saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving spot IDs: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_spot_ids_to_drive_test(spot_ids):\n",
        "    \"\"\"Save spot IDs with a counter for uniqueness.\"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids_test'\n",
        "    os.makedirs(drive_folder, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    # Find existing files and determine the next counter\n",
        "    existing_files = [f for f in os.listdir(drive_folder) if f.startswith('spot_ids_') and f.endswith('.json')]\n",
        "    next_counter = len(existing_files) + 1\n",
        "\n",
        "    # Create a filename using a counter\n",
        "    file_path = os.path.join(drive_folder, f'spot_ids_{next_counter:04d}.json')\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(spot_ids, f, indent=4)\n",
        "        print(f\"Spot IDs successfully saved to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving spot IDs: {e}\")\n",
        "        return None\n",
        "\n",
        "def store_spot_ids_to_drive_all(spot_ids):\n",
        "    \"\"\"Store the given spot IDs in a JSON file in Google Drive within one unified dictionary.\"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids_all'\n",
        "    os.makedirs(drive_folder, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    file_path = os.path.join(drive_folder, 'spot_ids.json')\n",
        "\n",
        "    try:\n",
        "        # Load existing data or initialize an empty dictionary\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                existing_data = json.load(f)\n",
        "        else:\n",
        "            existing_data = {}\n",
        "\n",
        "        # Append or update data without overwriting keys\n",
        "        for key, value in spot_ids.items():\n",
        "            if key in existing_data:\n",
        "                print(f\"Key {key} already exists. Skipping.\")\n",
        "            else:\n",
        "                existing_data[key] = value\n",
        "\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(existing_data, f, indent=4)\n",
        "        print(f\"Spot IDs successfully stored to {file_path}\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing spot IDs: {e}\")\n",
        "        return None\n",
        "\n",
        "def retrieve_all_spot_ids_from_drive_all():\n",
        "    \"\"\"Retrieve all stored spot IDs from Google Drive.\"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids_all'\n",
        "    file_path = os.path.join(drive_folder, 'spot_ids.json')\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(\"No spot IDs found.\")\n",
        "            return {}\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            spot_ids = json.load(f)\n",
        "            print(f\"Successfully retrieved {len(spot_ids)} spot ID entries.\")\n",
        "            return spot_ids\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving spot IDs: {e}\")\n",
        "        return {}\n",
        "\n",
        "def compare_focus(focus_list1, focus_list2):\n",
        "    \"\"\"\n",
        "    Compare two lists of focus values and print whether they are the same or different.\n",
        "    \"\"\"\n",
        "    if len(focus_list1) != len(focus_list2):\n",
        "        print(\"❗ The focus lists have different lengths. They are not the same.\")\n",
        "        return\n",
        "\n",
        "    for i, (f1, f2) in enumerate(zip(focus_list1, focus_list2)):\n",
        "        if np.array_equal(f1, f2):\n",
        "            print(f\"✅ Focus {i + 1} is the same.\")\n",
        "        else:\n",
        "            print(f\"❗ Focus {i + 1} is different.\")\n",
        "           # print(f\"  Focus 1: {f1}\")\n",
        "           # print(f\"  Focus 2: {f2}\")\n",
        "\n",
        "def visualize_processed_pixels(focused_regions):\n",
        "    \"\"\"\n",
        "    Visualizes the processed portions from both input images side by side.\n",
        "    \"\"\"\n",
        "    num_regions = len(focused_regions)\n",
        "    if num_regions == 0:\n",
        "        print(\"No processed regions to visualize.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for i in range(num_regions):\n",
        "        # Reshape to 2D (Assuming 32x32 images)\n",
        "        reshaped_image = np.array(focused_regions[i], dtype=np.uint8).reshape(16, 16)\n",
        "\n",
        "        plt.subplot(1, num_regions, i + 1)\n",
        "        plt.imshow(reshaped_image, cmap='gray')\n",
        "        plt.title(f\"Region {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Processed Image Regions\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_importance_map(image, importance_map):\n",
        "    \"\"\"\n",
        "    Visualize the importance map overlaid on the original image.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Importance Map\")\n",
        "    plt.imshow(importance_map, cmap='hot')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Important Regions Overlay\")\n",
        "    # Create a 3-channel image for color overlay\n",
        "    overlay = np.stack([\n",
        "        np.where(importance_map > 0, 255, image),  # Red channel - highlight important areas\n",
        "        image,  # Green channel\n",
        "        image   # Blue channel\n",
        "    ], axis=-1)\n",
        "\n",
        "    plt.imshow(overlay)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "image_path = 'input image :'\n",
        "image_path1 = 'input image :' # testing image to compare the training and testing\n",
        "training = True\n",
        "testing = False\n",
        "focus = []\n",
        "focus2_ = []\n",
        "\n",
        "if training == True:\n",
        "    image = image_to_pixels(image_path)\n",
        "    image1 = image_to_pixels(image_path1)\n",
        "    focus, focus2 = focusing_agent(image, image1, training = True)\n",
        "    compare_focus(focus, focus2)\n",
        "\n",
        "    for focus_ in focus2: # for testing purpose\n",
        "      focus__ = list_to_reversible_id(focus_)\n",
        "      focus2_.append(focus__)\n",
        "      save_spot_ids_to_drive_test(focus__)\n",
        "\n",
        "    focus_ids = []\n",
        "    for focus1 in focus:\n",
        "        focus_id = list_to_reversible_id(focus1)\n",
        "        focus_ids.append(focus_id)\n",
        "        save_spot_ids_to_drive(focus_id)\n",
        "\n",
        "    print(\"id comparison\")\n",
        "    compare_focus(focus_ids, focus2_)\n",
        "    print(\"test visualization\")\n",
        "    visualize_processed_pixels(focus2)\n",
        "\n",
        "    store_spot_ids_to_drive_all(spot_ids)\n",
        "\n",
        "if testing == True:\n",
        "    image = image_to_pixels(image_path)\n",
        "    image1 = image_to_pixels(image_path1)\n",
        "    importance_map = generate_importance_map(image)\n",
        "    visualize_importance_map(image, importance_map)\n",
        "\n",
        "    result = focusing_agent(image, image1, importance_map, testing=True)\n",
        "\n",
        "    if result:\n",
        "        focus1, focus2 = result\n",
        "        focus2_ = []  # Reset the list before appending\n",
        "\n",
        "        for focus_ in focus1: # For testing purpose\n",
        "            focus__ = list_to_reversible_id(focus_.flatten().tolist())\n",
        "            focus2_.append(focus__)\n",
        "            save_spot_ids_to_drive_test(focus__)\n",
        "\n",
        "        print(\"Test visualization\")\n",
        "        visualize_processed_pixels(focus2)\n",
        "    else:\n",
        "        print(\"No familiar spots to process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ring Tree\n",
        "the successful patterns/ID gets stored in a comman directory where ring tree can access these stored files and organize them in a ring tree structure, and assists in **storage**, **retrieval**, **removal of stored unwanted information**\n",
        "\n",
        "\n",
        "STORAGE - On training, the Ring tree function takes a successful patterns and organize them in a ring tree structure\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "RETRIEVAL - on testing, the model takes the input signals and retrieves the stored information\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "REMOVAL OF STORED INFORMATION - the ring tree function inherently works on cleaning the stored unwanted process. it does this process automatically.\n",
        "\n",
        "**DETAILS ABOUT THE RING TREE**\n",
        "\n",
        "*General rules*\n",
        "\n",
        "*   if branched, the dual process should get more info to confirm the path to travel\n",
        "*  searcher should get more input (atleast 5) to confirm the ring tree specificity to the input.\n",
        "*   initially give 2 outputs for each confirmation of nodes with the input, after confrimation happens for 3 consecutive nodes, increase the output number to 4.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Specific rules*\n",
        "1. Storage of ring tree - should be sequentially one by one. output should be given by travelling down the tree\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. Branching - only if present in main root.\n",
        "\n",
        "*   should check for atleast 3 rings similarity to the 3 consecutive inputs to confirm that main root ring to create as branch.\n",
        "*   if atleast 5 rings are same then directy create connection within the main root, no need of branch.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. Insertion of new ring - only if not present in main root + not found when cross-ring connection\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4. Cross ring connection - only if donar ring tree is activated by input source. each activated ring from donar ring tree should be taken for testing similarity with the receiver ring tree activated leftalone ring to find a good match and then proceed with the receiver ring tree processing. leave the donar tree\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "5. Dual process - should actively communicate with focusing agent to gather additional input\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TRAINING** - Feeds all the piece of information into the model.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**ORGANIZING** - Organizes the incoming already familiar information within the ring tree netork\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TESTING** - A function that enables comprehensive traversal of interconnected 'ring trees' based on incoming input and external confirmation via focusing agent."
      ],
      "metadata": {
        "id": "rTDTMZpmKwgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RING TREE\n",
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import base64\n",
        "import zlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "output2 = []\n",
        "importance_map = generate_importance_map(image)\n",
        "\n",
        "def ring_tree(data=None, training=False, organizing=False, testing=False, continuation=False, previous_result=None, **kwargs):\n",
        "    # Load existing state when the function is first called\n",
        "    if not hasattr(ring_tree, 'surface'):\n",
        "        # Try to load existing state from drive\n",
        "        load_ring_tree_from_drive()\n",
        "\n",
        "        # If loading fails or no previous state, initialize\n",
        "        if not hasattr(ring_tree, 'surface'):\n",
        "            ring_tree.surface = {}\n",
        "            ring_tree.surface_rankings = {\"R'\": {}, \"R''\": {}, \"R'''\": {}}\n",
        "            ring_tree.access_counts = {}\n",
        "            ring_tree.windows = {}\n",
        "            ring_tree.branches = {}\n",
        "            ring_tree.branch_connections = {}\n",
        "            ring_tree.tree_counter = 1\n",
        "            ring_tree.branch_counter = 1\n",
        "            ring_tree.current_tree = None\n",
        "\n",
        "        ring_tree.min_trees_for_ranking = 10\n",
        "        ring_tree.activation_thresholds = {\"detach\": 10}\n",
        "\n",
        "    def compare_values(val1, val2):\n",
        "        if isinstance(val1, tuple) and isinstance(val2, tuple):\n",
        "            if len(val1) != len(val2):\n",
        "                return False\n",
        "            return all(abs(a - b) < 0.001 for a, b in zip(val1, val2))\n",
        "        return val1 == val2\n",
        "\n",
        "    def dict_match(dict1, dict2):\n",
        "\n",
        "        # Directly compare if both are not dictionaries\n",
        "        if not isinstance(dict1, dict) and not isinstance(dict2, dict):\n",
        "            return dict1 == dict2\n",
        "\n",
        "        # Ensure both are dictionaries for further checks\n",
        "        if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
        "            return False\n",
        "\n",
        "        # Check if keys match\n",
        "        if set(dict1.keys()) != set(dict2.keys()):\n",
        "            return False\n",
        "\n",
        "        # Check values using compare_values\n",
        "        for key in dict1:\n",
        "            if key not in dict2 or not compare_values(dict1[key], dict2[key]):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def update_rankings():\n",
        "        \"\"\"Update rankings of trees based on access patterns\"\"\"\n",
        "        # Only use ranking system if we have enough trees\n",
        "        if len(ring_tree.surface) < ring_tree.min_trees_for_ranking:\n",
        "            return \"Not enough trees for ranking system\"\n",
        "\n",
        "        # Calculate total access for each tree\n",
        "        tree_access = {}\n",
        "        for tree_name, counts in ring_tree.access_counts.items():\n",
        "            if tree_name in ring_tree.surface:  # Only consider existing trees\n",
        "                tree_access[tree_name] = sum(counts)\n",
        "\n",
        "        # Sort trees by total access\n",
        "        sorted_trees = sorted(tree_access.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Reset rankings\n",
        "        for rank in ring_tree.surface_rankings:\n",
        "            ring_tree.surface_rankings[rank] = []\n",
        "\n",
        "        # Distribute trees into ranking buckets\n",
        "        total_trees = len(sorted_trees)\n",
        "        r_prime_count = max(1, total_trees // 3)  # At least 1 tree in R'\n",
        "        r_double_prime_count = max(1, total_trees // 3)  # At least 1 tree in R''\n",
        "\n",
        "        # Assign R' (top third or at least 1)\n",
        "        for i in range(min(r_prime_count, len(sorted_trees))):\n",
        "            ring_tree.surface_rankings[\"R'\"].append(sorted_trees[i][0])\n",
        "\n",
        "        # Assign R'' (middle third or at least 1)\n",
        "        for i in range(r_prime_count, min(r_prime_count + r_double_prime_count, len(sorted_trees))):\n",
        "            ring_tree.surface_rankings[\"R''\"].append(sorted_trees[i][0])\n",
        "\n",
        "        # Assign R''' (bottom third or remaining)\n",
        "        for i in range(r_prime_count + r_double_prime_count, len(sorted_trees)):\n",
        "            ring_tree.surface_rankings[\"R'''\"].append(sorted_trees[i][0])\n",
        "\n",
        "        r1 = len(ring_tree.surface_rankings[\"R'\"])\n",
        "        r2 = len(ring_tree.surface_rankings[\"R''\"])\n",
        "        r3 = len(ring_tree.surface_rankings[\"R'''\"])\n",
        "        return f\"Updated rankings: R': {r1} trees, R'': {r2} trees, R''': {r3} trees\"\n",
        "\n",
        "    def find_matching_prefix(input_data, min_prefix=2):\n",
        "        \"\"\"Find the longest matching prefix between input_data and any existing tree\"\"\"\n",
        "        best_match = (None, -1, 0)  # (tree_name, start_idx, match_length)\n",
        "        # If we have enough trees to use ranking system, prioritize by rank\n",
        "        if len(ring_tree.surface) > 0:\n",
        "            for rank in [\"R'\", \"R''\", \"R'''\"]:\n",
        "                for tree_name in ring_tree.surface_rankings.get(rank, []):\n",
        "                    tree_data = ring_tree.surface.get(tree_name, [])\n",
        "                    # Check if the beginning of input_data matches beginning of tree_data\n",
        "                    match_length = 0\n",
        "\n",
        "                    for j in range(len(tree_data)):\n",
        "                       # print(f\"Comparing input_data[{j}] with all tree data\")\n",
        "                        for i in range(2):\n",
        "\n",
        "                            if dict_match(input_data[i], tree_data[j]):\n",
        "                                match_length += 1\n",
        "                            else:\n",
        "                                pass\n",
        "\n",
        "                    if match_length >= min_prefix:\n",
        "                        best_match = (tree_name, 0, match_length)\n",
        "                        # Early return for efficiency if we find a match in high-ranked trees\n",
        "                        if rank == \"R'\":\n",
        "                            return best_match\n",
        "\n",
        "    def find_return_point(branch_data, main_tree):\n",
        "        \"\"\"Find where the branch can return to the main tree\"\"\"\n",
        "        tree_data = ring_tree.surface[main_tree]\n",
        "\n",
        "        # Check the last node in branch data\n",
        "        last_node = branch_data[-1]\n",
        "        for idx, node in enumerate(tree_data):\n",
        "            if dict_match(last_node, node):\n",
        "                # Found a matching node - return to tree from this point\n",
        "                return idx, idx + 1\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def create_branch(tree_name, initial_branch_data, common_prefix_length):\n",
        "        branch_name = f\"branch_{ring_tree.branch_counter}\"\n",
        "        ring_tree.branch_counter += 1\n",
        "\n",
        "        # Initialize accumulated data with the initial branch data\n",
        "        accumulated_data = initial_branch_data.copy()\n",
        "        continue_gathering = True\n",
        "\n",
        "        # Keep gathering data until we find something that doesn't match the main tree\n",
        "        while continue_gathering:\n",
        "            # Call focusing_agent to get additional data\n",
        "            result = focusing_agent(image, image1, dual_process=True, request_input=accumulated_data[-1])\n",
        "\n",
        "            if isinstance(result, tuple) and len(result) == 2:\n",
        "                additional_data, boolean_flag = result\n",
        "                print(f\"Additional data: {additional_data}, Boolean Flag: {boolean_flag}\")\n",
        "            else:\n",
        "                boolean_flag = result\n",
        "                print(f\"Boolean Flag: {boolean_flag}\")\n",
        "                additional_data = None\n",
        "\n",
        "            if boolean_flag is None or additional_data is None:\n",
        "                print(\"No more additional data. Ending data gathering.\")\n",
        "                break\n",
        "\n",
        "            # Check if additional data exists in main tree\n",
        "            data_in_main_tree = False\n",
        "           # for node in additional_data:\n",
        "            for tree_data_node in ring_tree.surface.get(tree_name, []):\n",
        "                if dict_match(additional_data, tree_data_node):\n",
        "                    data_in_main_tree = True\n",
        "                    break\n",
        "\n",
        "            # Add the additional_data as a whole item, not character by character\n",
        "            if isinstance(additional_data, str):\n",
        "                accumulated_data.append(additional_data)\n",
        "            elif isinstance(additional_data, list):\n",
        "                accumulated_data.extend(additional_data)\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected type for additional_data: {type(additional_data)}\")\n",
        "                # Try to add it anyway\n",
        "                accumulated_data.append(additional_data)\n",
        "\n",
        "            if data_in_main_tree:\n",
        "                # Data matches with main tree, continue gathering\n",
        "                print(f\"Data found in main tree. Continuing to gather more data.\")\n",
        "            else:\n",
        "                # Found data that doesn't match main tree, stop gathering\n",
        "                print(f\"Data not found in main tree. Stopping data gathering.\")\n",
        "                continue_gathering = False\n",
        "\n",
        "        # Now create the branch with all accumulated data\n",
        "        # Only now find return path to tree\n",
        "        return_from_idx, return_to_idx = find_return_point(accumulated_data, tree_name)\n",
        "        # Store branch info\n",
        "        ring_tree.branches[branch_name] = {\n",
        "            \"reference_tree\": tree_name,\n",
        "            \"reference_length\": common_prefix_length,\n",
        "            \"unique_data\": accumulated_data,\n",
        "            \"returns_to_tree\": return_to_idx is not None,\n",
        "            \"return_from_idx\": return_from_idx,\n",
        "            \"return_to_idx\": return_to_idx\n",
        "        }\n",
        "\n",
        "        # Store connection information\n",
        "        branch_off_point = common_prefix_length - 1\n",
        "        ring_tree.branch_connections[branch_name] = {\n",
        "            \"parent_tree\": tree_name,\n",
        "            \"branch_point_index\": branch_off_point,\n",
        "            \"returns_to_tree\": return_to_idx is not None,\n",
        "            \"return_from_branch_idx\": len(accumulated_data) - 1 if return_to_idx is not None else None,\n",
        "            \"return_to_tree_idx\": return_to_idx\n",
        "        }\n",
        "\n",
        "        message = f\"Created branch {branch_name} from {tree_name}\\n\"\n",
        "        message += f\"Common prefix: {accumulated_data}\\n\"\n",
        "        message += f\"Branch-specific data: {accumulated_data[common_prefix_length:]}\\n\"\n",
        "\n",
        "        if return_to_idx is not None:\n",
        "            message += f\"Branch returns to tree at index {return_to_idx}\\n\"\n",
        "\n",
        "        return message\n",
        "\n",
        "    def update_tree_with_new_nodes(tree_name, match_length, data):\n",
        "        \"\"\"Update the specified tree with new nodes from data, inserting at the appropriate position\"\"\"\n",
        "        # Get the existing tree data\n",
        "        tree_data = ring_tree.surface[tree_name]\n",
        "\n",
        "        # Determine which nodes need to be added (those beyond match_length)\n",
        "        nodes_to_add = data[match_length:]\n",
        "\n",
        "        # Calculate insertion position (right after the matching prefix)\n",
        "        insertion_position = match_length\n",
        "\n",
        "        # Insert the new nodes at the specified position\n",
        "        for i, node in enumerate(nodes_to_add):\n",
        "            tree_data.insert(insertion_position + i, node)\n",
        "            # Also update the access counts for the new nodes\n",
        "            ring_tree.access_counts[tree_name].insert(insertion_position + i, 0)\n",
        "\n",
        "        # Update the indices for any window pointers or other references\n",
        "        if ring_tree.windows[tree_name] >= insertion_position:\n",
        "            ring_tree.windows[tree_name] += len(nodes_to_add)\n",
        "\n",
        "        # Update connection indices that might be affected by the insertion\n",
        "        for conn_name, conn_data in ring_tree.branch_connections.items():\n",
        "            if conn_data[\"parent_tree\"] == tree_name and conn_data[\"branch_point_index\"] >= insertion_position:\n",
        "                conn_data[\"branch_point_index\"] += len(nodes_to_add)\n",
        "\n",
        "            # Update similarity nodes indices if needed\n",
        "            for i, sim_node in enumerate(conn_data.get(\"similarity_nodes\", [])):\n",
        "                idx, sim_tree, sim_idx = sim_node\n",
        "                if sim_tree == tree_name and sim_idx >= insertion_position:\n",
        "                    conn_data[\"similarity_nodes\"][i] = (idx, sim_tree, sim_idx + len(nodes_to_add))\n",
        "\n",
        "        # Return information about the update\n",
        "        return f\"Inserted {len(nodes_to_add)} new nodes at position {insertion_position} in {tree_name}\"\n",
        "\n",
        "    def update_access_counts(tree_name, node_index):\n",
        "        \"\"\"Update access counts for a node and check if window should be moved\"\"\"\n",
        "        if tree_name in ring_tree.access_counts:\n",
        "            # Ensure access_counts array is large enough\n",
        "            if len(ring_tree.access_counts[tree_name]) <= node_index:\n",
        "                # Extend the array to accommodate the node index\n",
        "                extension = [0] * (node_index - len(ring_tree.access_counts[tree_name]) + 1)\n",
        "                ring_tree.access_counts[tree_name].extend(extension)\n",
        "\n",
        "            # Now we can safely increment the access count\n",
        "            ring_tree.access_counts[tree_name][node_index] += 1\n",
        "\n",
        "            # Update window to most frequently accessed node\n",
        "            max_access = max(ring_tree.access_counts[tree_name])\n",
        "            max_index = ring_tree.access_counts[tree_name].index(max_access)\n",
        "            old_window = ring_tree.windows.get(tree_name, 0)\n",
        "            ring_tree.windows[tree_name] = max_index\n",
        "\n",
        "            # Check if access count exceeds threshold for detaching\n",
        "            if ring_tree.access_counts[tree_name][max_index] >= ring_tree.activation_thresholds[\"detach\"]:\n",
        "                return detach_window_to_new_tree(tree_name, max_index)\n",
        "\n",
        "            return f\"Updated window for {tree_name} from {old_window} to {max_index}, access count now: {ring_tree.access_counts[tree_name][node_index]}\"\n",
        "        return \"No access count updated - tree not found\"\n",
        "\n",
        "    def detach_window_to_new_tree(tree_name, window_index):\n",
        "        \"\"\"\n",
        "        Detach a window of continuous activated nodes and all following nodes\n",
        "        from the original tree and create a new single tree with them.\n",
        "        \"\"\"\n",
        "        if tree_name not in ring_tree.surface:\n",
        "            return f\"Tree {tree_name} not found\"\n",
        "\n",
        "        tree_data = ring_tree.surface[tree_name]\n",
        "        access_counts = ring_tree.access_counts[tree_name]\n",
        "        tree_size = len(tree_data)\n",
        "\n",
        "        # Find a continuous sequence of activated nodes (3-5 nodes)\n",
        "        # Start with the most activated node\n",
        "        window_start = window_index\n",
        "        window_end = window_index + 1\n",
        "\n",
        "        # Look for activated nodes before the window_index (up to 2 positions)\n",
        "        for i in range(1, 3):\n",
        "            prev_idx = (window_index - i) % tree_size\n",
        "            # Only include if it's consecutive (physically adjacent) and has been activated\n",
        "            if prev_idx == (window_index - i) and access_counts[prev_idx] > 0:\n",
        "                window_start = prev_idx\n",
        "            else:\n",
        "                break  # Stop if we hit a non-activated node or non-consecutive position\n",
        "\n",
        "        # Look for activated nodes after the window_index (up to 2 positions)\n",
        "        for i in range(1, 3):\n",
        "            next_idx = (window_index + i) % tree_size\n",
        "            # Only include if it's consecutive and has been activated\n",
        "            if next_idx == (window_index + i) and access_counts[next_idx] > 0:\n",
        "                window_end = next_idx + 1\n",
        "            else:\n",
        "                break  # Stop if we hit a non-activated node or non-consecutive position\n",
        "\n",
        "        # Check if we have at least 3 continuous nodes in our window\n",
        "        if window_end - window_start < 3:\n",
        "            return f\"Not enough continuous activated nodes around {window_index} in {tree_name} to form a window\"\n",
        "\n",
        "        # Create a new tree from the window area plus all following nodes\n",
        "        new_tree_name = f\"tree_{ring_tree.tree_counter}_from_{tree_name}\"\n",
        "        ring_tree.tree_counter += 1\n",
        "\n",
        "        # Copy nodes from the window start to the end of the tree\n",
        "        # This includes both the activated window and all following nodes\n",
        "        new_tree_data = tree_data[window_start:]\n",
        "\n",
        "        # Store as a new tree\n",
        "        ring_tree.surface[new_tree_name] = new_tree_data\n",
        "\n",
        "        # Initialize access counts for new tree\n",
        "        ring_tree.access_counts[new_tree_name] = [0] * len(new_tree_data)\n",
        "        ring_tree.windows[new_tree_name] = 0\n",
        "\n",
        "        # Update the original tree to only contain nodes before the window\n",
        "        ring_tree.surface[tree_name] = tree_data[:window_start]\n",
        "        ring_tree.access_counts[tree_name] = access_counts[:window_start]\n",
        "\n",
        "        # Add new tree to highest rank if ranking system is active\n",
        "        if len(ring_tree.surface) >= ring_tree.min_trees_for_ranking:\n",
        "            if new_tree_name not in ring_tree.surface_rankings[\"R'\"]:\n",
        "                ring_tree.surface_rankings[\"R'\"].append(new_tree_name)\n",
        "\n",
        "        update_rankings()\n",
        "\n",
        "        result_message = (f\"Detached window of {window_end - window_start} continuous nodes \"\n",
        "                        f\"plus {len(new_tree_data) - (window_end - window_start)} following nodes \"\n",
        "                        f\"to new tree {new_tree_name}. \"\n",
        "                        f\"Original tree {tree_name} now has {len(ring_tree.surface[tree_name])} nodes\")\n",
        "\n",
        "        return result_message\n",
        "\n",
        "    def consecutive_match_(tree_data, data, index):\n",
        "        # Check for 3 consecutive matches\n",
        "        for i in range(3):\n",
        "            compare_index = (index + i) % len(tree_data)\n",
        "            data_index = i % len(data)\n",
        "            if not dict_match(tree_data[compare_index], data[data_index]):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def searching_via_common_information(data):\n",
        "        # Call the focusing agent twice to check activation\n",
        "        donar_tree = None\n",
        "        for _ in range(2):\n",
        "            focus, focus2 = focusing_agent(image, image1, training=True)\n",
        "            focus_ids = [list_to_reversible_id(f) for f in focus]\n",
        "\n",
        "            # Search for matches in high-ranked ring trees\n",
        "            for rank in [\"R'\", \"R''\", \"R'''\"]:\n",
        "                for tree_name in ring_tree.surface_rankings.get(rank, []):\n",
        "                    tree_data = ring_tree.surface.get(tree_name, [])\n",
        "\n",
        "                    # Check for 3 consecutive matches using focus_ids\n",
        "                    for index in range(len(tree_data) - 2):\n",
        "                        if consecutive_match_(tree_data, focus_ids, index):\n",
        "                            donar_tree = tree_name\n",
        "                            break\n",
        "                    if donar_tree:\n",
        "                        break\n",
        "                if donar_tree:\n",
        "                    break\n",
        "\n",
        "            if donar_tree:\n",
        "                break  # Stop checking if a donor tree is found\n",
        "\n",
        "        if not donar_tree:\n",
        "            return False  # No donor tree activated, try another method\n",
        "\n",
        "        # Find receiver tree using existing matching logic\n",
        "        receiver_tree, start_idx, match_length = find_matching_prefix(data, min_prefix=3)\n",
        "\n",
        "        if not receiver_tree or receiver_tree == donar_tree:\n",
        "            return \"Receiver tree not found or same as donar tree.\"\n",
        "\n",
        "        # Create connections for nodes that exist in other trees\n",
        "        connections_created = []\n",
        "\n",
        "        for i in range(match_length, len(data)):\n",
        "            node = data[i]\n",
        "            found_in_donar = None\n",
        "            found_in_receiver = None\n",
        "\n",
        "            # Search for node in donar tree\n",
        "            for index, existing_node in enumerate(ring_tree.surface[donar_tree]):\n",
        "                if dict_match(node, existing_node):\n",
        "                    found_in_donar = index\n",
        "                    break\n",
        "\n",
        "            # Search for node in receiver tree\n",
        "            for index, existing_node in enumerate(ring_tree.surface[receiver_tree]):\n",
        "                if dict_match(node, existing_node):\n",
        "                    found_in_receiver = index\n",
        "                    break\n",
        "\n",
        "            if found_in_donar is not None and found_in_receiver is None:\n",
        "                # Connect from donar to receiver\n",
        "                our_idx = match_length + (i - match_length)\n",
        "                connection_name = f\"connection_{ring_tree.branch_counter}\"\n",
        "                ring_tree.branch_counter += 1\n",
        "\n",
        "                ring_tree.branch_connections[connection_name] = {\n",
        "                    \"parent_tree\": receiver_tree,\n",
        "                    \"branch_point_index\": our_idx,\n",
        "                    \"connected_tree\": donar_tree,\n",
        "                    \"connected_index\": found_in_donar,\n",
        "                    \"connection_type\": 'cross_tree',\n",
        "                    \"similarity_nodes\": [(i, donar_tree, found_in_donar)]\n",
        "                }\n",
        "\n",
        "                # Create reverse connection\n",
        "                connection_name2 = f\"connection_{ring_tree.branch_counter}\"\n",
        "                ring_tree.branch_counter += 1\n",
        "\n",
        "                ring_tree.branch_connections[connection_name2] = {\n",
        "                    \"parent_tree\": donar_tree,\n",
        "                    \"branch_point_index\": found_in_donar,\n",
        "                    \"connected_tree\": receiver_tree,\n",
        "                    \"connected_index\": our_idx,\n",
        "                    \"connection_type\": 'cross_tree',\n",
        "                    \"similarity_nodes\": [(i, donar_tree, found_in_donar)]\n",
        "                }\n",
        "\n",
        "                connections_created.append((receiver_tree, our_idx, donar_tree, found_in_donar))\n",
        "\n",
        "        if connections_created:\n",
        "            connections_str = \", \".join([f\"{c[0]}[{c[1]}]↔{c[2]}[{c[3]}]\" for c in connections_created])\n",
        "            return f\"Connections established: {connections_str}\"\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # Training mode: create and populate trees\n",
        "    if training and data:\n",
        "        if ring_tree.current_tree is None or len(ring_tree.surface.get(ring_tree.current_tree, [])) >= 20:\n",
        "            ring_tree.current_tree = f\"tree_{ring_tree.tree_counter}\"\n",
        "            ring_tree.tree_counter += 1\n",
        "            ring_tree.surface[ring_tree.current_tree] = []\n",
        "            #ring_tree.surface_rankings[\"R'\"][ring_tree.current_tree] = {}\n",
        "            ring_tree.access_counts[ring_tree.current_tree] = []\n",
        "            ring_tree.windows[ring_tree.current_tree] = 0\n",
        "\n",
        "        ring_tree.surface[ring_tree.current_tree].append(data)\n",
        "        if \"R'\" not in ring_tree.surface_rankings:\n",
        "            ring_tree.surface_rankings[\"R'\"] = {}\n",
        "\n",
        "        if ring_tree.current_tree not in ring_tree.surface_rankings[\"R'\"]:\n",
        "            ring_tree.surface_rankings[\"R'\"][ring_tree.current_tree] = []\n",
        "\n",
        "        ring_tree.surface_rankings[\"R'\"][ring_tree.current_tree].append(data)\n",
        "\n",
        "        ring_tree.access_counts[ring_tree.current_tree].append(0)\n",
        "\n",
        "        # Check if ranking system should be activated\n",
        "        if len(ring_tree.surface) == ring_tree.min_trees_for_ranking:\n",
        "            update_rankings()\n",
        "            message = f\"Stored in {ring_tree.current_tree}: {data} (Item {len(ring_tree.surface[ring_tree.current_tree])} of 5). Ranking system activated!\"\n",
        "        else:\n",
        "            message = f\"Stored in {ring_tree.current_tree}: {data} (Item {len(ring_tree.surface[ring_tree.current_tree])} of 5)\"\n",
        "\n",
        "            # If we already have enough trees, update rankings\n",
        "            if len(ring_tree.surface) > ring_tree.min_trees_for_ranking:\n",
        "                update_rankings()\n",
        "\n",
        "        return message\n",
        "\n",
        "    # Organizing mode: handle the sequence according to the logic\n",
        "    if organizing and data:\n",
        "\n",
        "        # Call focusing_agent to get additional data if needed\n",
        "        result = focusing_agent(image, image1, dual_process=True, request_input=data)\n",
        "=\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            data_2, boolean_flag = result\n",
        "            print(f\"Data 2: {data_2}, Boolean Flag: {boolean_flag}\")\n",
        "        else:\n",
        "            boolean_flag = result\n",
        "            print(f\"Boolean Flag: {boolean_flag}\")\n",
        "        if boolean_flag is None:\n",
        "            print(\"Boolean flag is None. Ending function.\")\n",
        "            return\n",
        "\n",
        "        # Combine data and data_2 for further processing\n",
        "        combined_data = [data, data_2]\n",
        "=\n",
        "        # Check if combined_data is already in the same order in the ring tree\n",
        "        def check_existing_order():\n",
        "            # Check main root\n",
        "            for tree_name in ring_tree.surface:\n",
        "                tree_data = ring_tree.surface[tree_name]\n",
        "\n",
        "                # Check if combined_data appears in sequence in tree_data\n",
        "                for i in range(len(tree_data) - len(combined_data) + 1):\n",
        "                    matches = True\n",
        "                    for j in range(len(combined_data)):\n",
        "                        if not dict_match(combined_data[j], tree_data[i + j]):\n",
        "                            matches = False\n",
        "                            break\n",
        "                    if matches:\n",
        "                        print(f\"Combined data already exists in the same order in tree: {tree_name}\")\n",
        "                        return True\n",
        "\n",
        "                # Check branches\n",
        "                for branch_name, branch_info in ring_tree.branches.items():\n",
        "                    if branch_info[\"reference_tree\"] == tree_name:\n",
        "                        # Reconstruct branch data\n",
        "                        branch_data = (tree_data[:branch_info[\"reference_length\"]] +\n",
        "                                      branch_info[\"unique_data\"])\n",
        "\n",
        "                        # Check if combined_data appears in sequence in branch_data\n",
        "                        for i in range(len(branch_data) - len(combined_data) + 1):\n",
        "                            matches = True\n",
        "                            for j in range(len(combined_data)):\n",
        "                                if not dict_match(combined_data[j], branch_data[i + j]):\n",
        "                                    matches = False\n",
        "                                    break\n",
        "                            if matches:\n",
        "                                print(f\"Combined data already exists in the same order in branch: {branch_name}\")\n",
        "                                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        # If data is already in correct order, end function\n",
        "        if check_existing_order():\n",
        "            return \"Data is already correctly ordered in the ring tree.\"\n",
        "\n",
        "        # Existing organizing logic for creating branch and node insertion\n",
        "        match_result = find_matching_prefix(combined_data, min_prefix=2)\n",
        "\n",
        "        if match_result is None:\n",
        "            print(\"No matching prefix found.\")\n",
        "            return\n",
        "        else:\n",
        "            tree_name, start_idx, match_length = match_result\n",
        "\n",
        "        if tree_name:\n",
        "            # In organizing mode:\n",
        "            should_create_branch = False\n",
        "            for i in range(0, 2):\n",
        "                node = combined_data[i]\n",
        "                # Check if any exact match exists in any tree\n",
        "                for tree_data2 in ring_tree.surface.get(tree_name, []):\n",
        "                    if dict_match(node, tree_data2):\n",
        "                        should_create_branch = True\n",
        "                        break\n",
        "                if should_create_branch:\n",
        "                    break\n",
        "\n",
        "            if should_create_branch:\n",
        "                # Create a branch only if we found exact matches of new nodes in existing trees\n",
        "                message = create_branch(tree_name, combined_data, match_length)\n",
        "            else:\n",
        "                # Try searching via common information before directly updating\n",
        "                common_information = searching_via_common_information(data)\n",
        "                if not common_information:\n",
        "                    message = update_tree_with_new_nodes(tree_name, match_length, data)\n",
        "            return message\n",
        "\n",
        "        else:\n",
        "            # No matching pattern found, create a new tree\n",
        "            new_tree_name = f\"tree_{ring_tree.tree_counter}\"\n",
        "            ring_tree.tree_counter += 1\n",
        "            ring_tree.surface[new_tree_name] = data.copy()\n",
        "            ring_tree.access_counts[new_tree_name] = [0] * len(data)\n",
        "            ring_tree.windows[new_tree_name] = 0\n",
        "\n",
        "            # Ensure ranking dictionary exists\n",
        "            if \"R'''\" not in ring_tree.surface_rankings:\n",
        "                ring_tree.surface_rankings[\"R'''\"] = []\n",
        "\n",
        "            # Add to lowest rank if ranking is active\n",
        "            if len(ring_tree.surface) >= ring_tree.min_trees_for_ranking:\n",
        "                ring_tree.surface_rankings[\"R'''\"].append(new_tree_name)\n",
        "                update_rankings()\n",
        "\n",
        "            message = f\"Created new tree {new_tree_name} with {len(data)} nodes.\"\n",
        "            return message\n",
        "\n",
        "    # Testing mode: find and retrieve data with confirmation mechanism\n",
        "    if testing and data:\n",
        "        result = {\"found\": None, \"message\": \"Data not found\", \"connected\": [], \"tree\": None, \"awaiting_confirmation\": True}\n",
        "\n",
        "        def consecutive_match(tree_data, data, index):\n",
        "            # Check for 2 consecutive matches\n",
        "            for i in range(2):\n",
        "                compare_index = (index + i) % len(tree_data)\n",
        "                data_index = i % len(data)\n",
        "                if not dict_match(tree_data[compare_index], data[data_index]):\n",
        "                    return False\n",
        "            return True\n",
        "\n",
        "        def get_next_search_index(start_index, connected_nodes, total_length):\n",
        "            # Determine the next search index, ensuring it is after the connected nodes\n",
        "            last_connected_index = max([start_index] + connected_nodes)\n",
        "            return (last_connected_index + 1) % total_length\n",
        "\n",
        "        # Initialize traversal state\n",
        "        state = {\n",
        "            \"current_tree\": None,\n",
        "            \"current_index\": 0,\n",
        "            \"visited_nodes\": set(),\n",
        "            \"path\": [],\n",
        "            \"matched_rings\": [],\n",
        "            \"window_updates\": []  # Track window updates during traversal\n",
        "        }\n",
        "\n",
        "        # First search in high-ranked trees if ranking is active\n",
        "        potential_matches = []\n",
        "        if len(ring_tree.surface) > 0:\n",
        "            for rank in [\"R'\", \"R''\", \"R'''\"]:\n",
        "                for tree_name in ring_tree.surface_rankings.get(rank, []):\n",
        "                    tree_data = ring_tree.surface.get(tree_name, [])\n",
        "\n",
        "                    for index, item in enumerate(tree_data):\n",
        "                        if dict_match(item, data):\n",
        "                            # Update access count for the matched node\n",
        "                            window_update = update_access_counts(tree_name, index)\n",
        "\n",
        "                            # Get initial connected nodes (just 2)\n",
        "                            connected = []\n",
        "                            for offset in range(1, 3):\n",
        "                                next_index = (index + offset) % len(tree_data)\n",
        "                                connected.append(tree_data[next_index])\n",
        "\n",
        "                            # Continue searching from the node after connected nodes\n",
        "                            next_search_index = get_next_search_index(index, [index + 1, index + 2], len(tree_data))\n",
        "\n",
        "                            potential_matches.append({\n",
        "                                \"found\": item,\n",
        "                                \"message\": f\"Found in {tree_name} ({rank} rank) at position {index}\",\n",
        "                                \"tree\": tree_name,\n",
        "                                \"connected\": connected,\n",
        "                                \"type\": \"tree\",\n",
        "                                \"index\": index,\n",
        "                                \"next_search_index\": next_search_index,\n",
        "                                \"window_update\": window_update\n",
        "                            })\n",
        "\n",
        "        # If we found potential matches, begin traversal with the first one\n",
        "        if potential_matches:\n",
        "            # Start with the first match\n",
        "            match = potential_matches[0]\n",
        "            state[\"current_tree\"] = match[\"tree\"]\n",
        "            state[\"current_index\"] = match[\"index\"]\n",
        "            state[\"matched_rings\"].append(match[\"found\"])\n",
        "            if \"window_update\" in match and match[\"window_update\"]:\n",
        "                state[\"window_updates\"].append(match[\"window_update\"])\n",
        "            # Begin traversal\n",
        "            try:\n",
        "                while True:\n",
        "                    # Get current tree data\n",
        "                    current_tree_data = ring_tree.surface.get(state[\"current_tree\"], [])\n",
        "                    if not current_tree_data:\n",
        "                        result = {\n",
        "                            \"status\": \"error\",\n",
        "                            \"message\": f\"Tree {state['current_tree']} is empty\",\n",
        "                            \"matched_rings\": state[\"matched_rings\"],\n",
        "                            \"path\": state[\"path\"],\n",
        "                            \"window_updates\": state[\"window_updates\"]\n",
        "                        }\n",
        "                        break\n",
        "\n",
        "                    # Check if the current node matches our search data\n",
        "                    current_node = current_tree_data[state[\"current_index\"]]\n",
        "                    output = id_to_list(current_node, 1024)\n",
        "                    output2.append(output)\n",
        "\n",
        "                    node_key = (state[\"current_tree\"], state[\"current_index\"])\n",
        "\n",
        "                    # Avoid infinite loops by tracking visited nodes\n",
        "                    if node_key in state[\"visited_nodes\"]:\n",
        "                        result = {\n",
        "                            \"status\": \"complete\",\n",
        "                            \"message\": \"Traversal complete - returned to previously visited node\",\n",
        "                            \"matched_rings\": state[\"matched_rings\"],\n",
        "                            \"path\": state[\"path\"],\n",
        "                            \"window_updates\": state[\"window_updates\"]\n",
        "                        }\n",
        "                        break\n",
        "\n",
        "                    state[\"visited_nodes\"].add(node_key)\n",
        "                    state[\"path\"].append({\"tree\": state[\"current_tree\"], \"index\": state[\"current_index\"], \"data\": current_node})\n",
        "\n",
        "                    # Update access count for this node\n",
        "                    window_update = update_access_counts(state[\"current_tree\"], state[\"current_index\"])\n",
        "                    if window_update:\n",
        "                        state[\"window_updates\"].append(window_update)\n",
        "\n",
        "                    # Check if there's a branch connection at this point\n",
        "                    branch_connection = None\n",
        "                    for conn_name, conn_data in ring_tree.branch_connections.items():\n",
        "                        if (conn_data[\"parent_tree\"] == state[\"current_tree\"] and\n",
        "                            conn_data[\"branch_point_index\"] == state[\"current_index\"]):\n",
        "                            branch_connection = {\"name\": conn_name, \"data\": conn_data}\n",
        "                            break\n",
        "\n",
        "                    if branch_connection:\n",
        "                        # Process branch connection\n",
        "                        branch_tree = branch_connection[\"data\"][\"connected_tree\"]\n",
        "                        branch_index = branch_connection[\"data\"][\"connected_index\"]\n",
        "\n",
        "                        # Get branch node(s)\n",
        "                        branch_tree_data = ring_tree.surface.get(branch_tree, [])\n",
        "                        if not branch_tree_data:\n",
        "                            result = {\n",
        "                                \"status\": \"error\",\n",
        "                                \"message\": f\"Branch tree {branch_tree} is empty\",\n",
        "                                \"matched_rings\": state[\"matched_rings\"],\n",
        "                                \"path\": state[\"path\"],\n",
        "                                \"window_updates\": state[\"window_updates\"]\n",
        "                            }\n",
        "                            break\n",
        "\n",
        "                        # For a single node branch, just verify it\n",
        "                        branch_node = branch_tree_data[branch_index]\n",
        "                        focus, focus2 = focusing_agent(image, image1, dual_process=True, requested_input=branch_node)\n",
        "\n",
        "                        if not focus2:\n",
        "                            result = {\n",
        "                                \"status\": \"error\",\n",
        "                                \"message\": f\"Branch node confirmation failed with focusing agent\",\n",
        "                                \"matched_rings\": state[\"matched_rings\"],\n",
        "                                \"path\": state[\"path\"],\n",
        "                                \"window_updates\": state[\"window_updates\"]\n",
        "                            }\n",
        "                            break\n",
        "\n",
        "                        # Add confirmed branch node to our path\n",
        "                        state[\"visited_nodes\"].add((branch_tree, branch_index))\n",
        "                        state[\"path\"].append({\"tree\": branch_tree, \"index\": branch_index, \"data\": branch_node})\n",
        "                        state[\"matched_rings\"].append(branch_node)\n",
        "                        print(f\"Confirmed node: {branch_node}\")  # Add this line to see the output\n",
        "                        # Call this when a node is confirmed\n",
        "                        output = id_to_list(branch_node, 1024)\n",
        "                        output2.append(output)\n",
        "                        # Update access count for branch node\n",
        "                        window_update = update_access_counts(branch_tree, branch_index)\n",
        "                        if window_update:\n",
        "                            state[\"window_updates\"].append(window_update)\n",
        "\n",
        "                        # Now traverse through branch tree\n",
        "                        state[\"current_tree\"] = branch_tree\n",
        "                        state[\"current_index\"] = branch_index\n",
        "                    else:\n",
        "                        # No branch, traverse to next node in current tree. no confirmation from focusing agent\n",
        "                        next_index = (state[\"current_index\"] + 1) % len(current_tree_data)\n",
        "                        next_node = current_tree_data[next_index]\n",
        "\n",
        "                        # Verify with focusing agent\n",
        "                        result = focusing_agent(image, image1, dual_process=True, request_input=next_node)\n",
        "\n",
        "                        if isinstance(result, tuple) and len(result) == 2:\n",
        "                            data_2, boolean_flag = result\n",
        "                            print(f\"Data 2: {data_2}, Boolean Flag: {boolean_flag}\")\n",
        "                        else:\n",
        "                            boolean_flag = result\n",
        "                            print(f\"Boolean Flag: {boolean_flag}\")\n",
        "\n",
        "                        if boolean_flag is None:\n",
        "                            result = {\n",
        "                                \"status\": \"error\",\n",
        "                                \"message\": f\"Focusing agent did not confirm next node in sequence\",\n",
        "                                \"error_location\": {\"tree\": state[\"current_tree\"], \"index\": next_index},\n",
        "                                \"matched_rings\": state[\"matched_rings\"],\n",
        "                                \"path\": state[\"path\"],\n",
        "                                \"window_updates\": state[\"window_updates\"]\n",
        "                            }\n",
        "                            break\n",
        "\n",
        "                        # Add confirmed node and update state\n",
        "                        state[\"current_index\"] = next_index\n",
        "                        state[\"matched_rings\"].append(next_node)\n",
        "                        print(f\"Confirmed node branch: {next_node}\")  # Add this line to see the output\n",
        "                        print(\"main root connection node\")\n",
        "                        output = id_to_list(next_node, 1024)\n",
        "                        output2.append(output)\n",
        "\n",
        "                        # Update access count for this node\n",
        "                        window_update = update_access_counts(state[\"current_tree\"], next_index)\n",
        "                        if window_update:\n",
        "                            state[\"window_updates\"].append(window_update)\n",
        "\n",
        "                    if len(state[\"matched_rings\"]) > 1:\n",
        "                        # Determine how many nodes to output next based on consecutive confirmations\n",
        "                        confirmation_count = len(state[\"matched_rings\"])\n",
        "                        if confirmation_count >= 6:\n",
        "                            nodes_to_output = 4  # Output 4 nodes after 4+ confirmations\n",
        "                        elif confirmation_count >= 4:\n",
        "                            nodes_to_output = 2  # Output 2 nodes after 2-3 confirmations\n",
        "                        else:\n",
        "                            nodes_to_output = 1  # Default to 1 node (shouldn't reach here due to condition)\n",
        "\n",
        "                        # Cap at 6 nodes maximum\n",
        "                        nodes_to_output = min(nodes_to_output, 6)\n",
        "\n",
        "                        # Get the next node to confirm with focusing agent\n",
        "                        next_index = (state[\"current_index\"] + 1) % len(current_tree_data)\n",
        "                        next_node = current_tree_data[next_index]\n",
        "\n",
        "                        result = focusing_agent(image, image1, importance_map, dual_process=True, request_input=next_node)\n",
        "\n",
        "                        if isinstance(result, tuple) and len(result) == 2:\n",
        "                            data_2, boolean_flag = result\n",
        "                            print(f\"Data 2: {data_2}, Boolean Flag: {boolean_flag}\")\n",
        "                        else:\n",
        "                            boolean_flag = result\n",
        "                            print(f\"Boolean Flag: {boolean_flag}\")\n",
        "\n",
        "                        if boolean_flag is None:\n",
        "                            result = {\n",
        "                                \"status\": \"error\",\n",
        "                                \"message\": f\"Focusing agent could not confirm next node in traversal\",\n",
        "                                \"error_location\": {\"tree\": state[\"current_tree\"], \"index\": next_index},\n",
        "                                \"matched_rings\": state[\"matched_rings\"],\n",
        "                                \"path\": state[\"path\"],\n",
        "                                \"window_updates\": state[\"window_updates\"]\n",
        "                            }\n",
        "                            break\n",
        "\n",
        "                        # Check if focus output matches any node in the current tree\n",
        "                        match_found = False\n",
        "                        for i, item in enumerate(current_tree_data):\n",
        "                            if dict_match(item, data_2):\n",
        "                                # This is the matched node\n",
        "                                state[\"current_index\"] = i\n",
        "                                state[\"matched_rings\"].append(item)\n",
        "\n",
        "                                # Output the confirmed matched node\n",
        "                                print(\"Confirmed matched node after initial\")\n",
        "                                output = id_to_list(item, 1024)\n",
        "                                output2.append(output)\n",
        "\n",
        "                                # Output the next nodes_to_output (1, 2, or 4) nodes attached to this matched node\n",
        "                                for offset in range(1, nodes_to_output + 1):\n",
        "                                    next_attached_index = (i + offset) % len(current_tree_data)\n",
        "                                    next_attached_node = current_tree_data[next_attached_index]\n",
        "                                    print(f\"Node {offset} after confirmed node\")\n",
        "                                    output = id_to_list(next_attached_node, 1024)\n",
        "                                    output2.append(output)\n",
        "\n",
        "                                # Continue traversal with the node after the last one shown\n",
        "                                next_node_index = (i + nodes_to_output + 1) % len(current_tree_data)\n",
        "                                state[\"current_index\"] = next_node_index\n",
        "\n",
        "                                # Update access count for the matched node\n",
        "                                window_update = update_access_counts(state[\"current_tree\"], i)\n",
        "                                if window_update:\n",
        "                                    state[\"window_updates\"].append(window_update)\n",
        "\n",
        "                                match_found = True\n",
        "                                break\n",
        "\n",
        "                        if not match_found:\n",
        "                            result = {\n",
        "                                \"status\": \"error\",\n",
        "                                \"message\": f\"No matching node found for focus output in any tree\",\n",
        "                                \"matched_rings\": state[\"matched_rings\"],\n",
        "                                \"path\": state[\"path\"],\n",
        "                                \"window_updates\": state[\"window_updates\"]\n",
        "                            }\n",
        "                            break\n",
        "\n",
        "            except Exception as e:\n",
        "                result = {\n",
        "                    \"status\": \"error\",\n",
        "                    \"message\": f\"Unexpected error during traversal: {str(e)}\",\n",
        "                    \"matched_rings\": state[\"matched_rings\"],\n",
        "                    \"path\": state[\"path\"],\n",
        "                    \"window_updates\": state[\"window_updates\"]\n",
        "                }\n",
        "        else:\n",
        "            result = {\"status\": \"error\", \"message\": \"Data not found\", \"matched_rings\": [], \"path\": [], \"window_updates\": []}\n",
        "\n",
        "        return result\n",
        "\n",
        "    # Print the current state if no specific operation is requested\n",
        "    if not any([training, organizing, testing]):\n",
        "        return {\n",
        "            \"surface\": ring_tree.surface,\n",
        "            \"surface_rankings\": ring_tree.surface_rankings,\n",
        "            \"access_counts\": ring_tree.access_counts,\n",
        "            \"windows\": ring_tree.windows,\n",
        "            \"branches\": ring_tree.branches,\n",
        "            \"branch_connections\": ring_tree.branch_connections,\n",
        "            \"visualization\": visualize_structures()\n",
        "        }\n",
        "\n",
        "def load_spot_ids_from_drive_with_waiting(wait_interval=5):\n",
        "    \"\"\"\n",
        "    Continuously load and return spot IDs from stored JSON files in counter order.\n",
        "    Remembers the last processed counter using a file.\n",
        "\n",
        "    Args:\n",
        "        wait_interval (int): Time (in seconds) to wait before checking again.\n",
        "\n",
        "    Yields:\n",
        "        dict: Loaded spot IDs from each processed file.\n",
        "    \"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids'\n",
        "    counter_file_path = '/content/drive/MyDrive/ring tree/last_processed_counter.txt'\n",
        "\n",
        "    if not os.path.exists(drive_folder):\n",
        "        print(f\"Directory not found at {drive_folder}\")\n",
        "        return\n",
        "\n",
        "    # Ensure counter file exists\n",
        "    if not os.path.exists(counter_file_path):\n",
        "        with open(counter_file_path, 'w') as f:\n",
        "            f.write('0')\n",
        "        print(\"Initialized last processed counter to 0.\")\n",
        "\n",
        "    # Read the last processed counter\n",
        "    with open(counter_file_path, 'r') as f:\n",
        "        last_processed_counter = int(f.read().strip())\n",
        "\n",
        "    while True:\n",
        "        # List all JSON files\n",
        "        json_files = [f for f in os.listdir(drive_folder) if f.startswith('spot_ids_') and f.endswith('.json')]\n",
        "\n",
        "        if not json_files:\n",
        "            print(\"No spot ID files found. Waiting...\")\n",
        "            time.sleep(wait_interval)\n",
        "            continue\n",
        "\n",
        "        # Sort files by counter value\n",
        "        json_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "\n",
        "        # Process only new files\n",
        "        for file_name in json_files:\n",
        "            file_path = os.path.join(drive_folder, file_name)\n",
        "            try:\n",
        "                # Extract counter from filename\n",
        "                current_counter = int(file_name.split('_')[2].split('.')[0])\n",
        "\n",
        "                if current_counter <= last_processed_counter:\n",
        "                    continue  # Skip already processed files\n",
        "\n",
        "                # Load and yield spot IDs\n",
        "                with open(file_path, 'r') as f:\n",
        "                    spot_ids = json.load(f)\n",
        "\n",
        "                print(f\"Processing {file_name}...\")\n",
        "\n",
        "                # Update the counter and delete file\n",
        "                last_processed_counter = current_counter\n",
        "                with open(counter_file_path, 'w') as f:\n",
        "                    f.write(str(last_processed_counter))\n",
        "\n",
        "                #os.remove(file_path)\n",
        "                print(f\"File {file_name} processed and deleted.\")\n",
        "\n",
        "                # Yield the loaded spot IDs for external processing\n",
        "                yield spot_ids\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "        # Wait for new files if no more to process\n",
        "        time.sleep(wait_interval)\n",
        "\n",
        "\n",
        "def load_spot_ids_from_drive_with_waiting_test(wait_interval=5):\n",
        "    \"\"\"\n",
        "    Continuously load and return spot IDs from stored JSON files in counter order.\n",
        "    Remembers the last processed counter using a file.\n",
        "\n",
        "    Args:\n",
        "        wait_interval (int): Time (in seconds) to wait before checking again.\n",
        "\n",
        "    Yields:\n",
        "        dict: Loaded spot IDs from each processed file.\n",
        "    \"\"\"\n",
        "    drive_folder = '/content/drive/MyDrive/ring tree/ids_test'\n",
        "    counter_file_path = '/content/drive/MyDrive/ring tree/last_processed_counter.txt'\n",
        "\n",
        "    if not os.path.exists(drive_folder):\n",
        "        print(f\"Directory not found at {drive_folder}\")\n",
        "        return\n",
        "\n",
        "    # Ensure counter file exists\n",
        "    if not os.path.exists(counter_file_path):\n",
        "        with open(counter_file_path, 'w') as f:\n",
        "            f.write('0')\n",
        "        print(\"Initialized last processed counter to 0.\")\n",
        "\n",
        "    # Read the last processed counter\n",
        "    with open(counter_file_path, 'r') as f:\n",
        "        last_processed_counter = int(f.read().strip())\n",
        "\n",
        "    while True:\n",
        "        # List all JSON files\n",
        "        json_files = [f for f in os.listdir(drive_folder) if f.startswith('spot_ids_') and f.endswith('.json')]\n",
        "\n",
        "        if not json_files:\n",
        "            print(\"No spot ID files found. Waiting...\")\n",
        "            time.sleep(wait_interval)\n",
        "            continue\n",
        "\n",
        "        # Sort files by counter value\n",
        "        json_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "\n",
        "        # Process only new files\n",
        "        for file_name in json_files:\n",
        "            file_path = os.path.join(drive_folder, file_name)\n",
        "            try:\n",
        "                # Extract counter from filename\n",
        "                current_counter = int(file_name.split('_')[2].split('.')[0])\n",
        "\n",
        "                if current_counter <= last_processed_counter:\n",
        "                    continue  # Skip already processed files\n",
        "\n",
        "                # Load and yield spot IDs\n",
        "                with open(file_path, 'r') as f:\n",
        "                    spot_ids = json.load(f)\n",
        "\n",
        "                print(f\"Processing {file_name}...\")\n",
        "\n",
        "                # Update the counter and delete file\n",
        "                last_processed_counter = current_counter\n",
        "                with open(counter_file_path, 'w') as f:\n",
        "                    f.write(str(last_processed_counter))\n",
        "\n",
        "                #os.remove(file_path)\n",
        "                print(f\"File {file_name} processed and deleted.\")\n",
        "\n",
        "                # Yield the loaded spot IDs for external processing\n",
        "                yield spot_ids\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "        # Wait for new files if no more to process\n",
        "        time.sleep(wait_interval)\n",
        "\n",
        "\n",
        "def save_ring_tree_to_drive():\n",
        "    \"\"\"Save the current ring tree structure to a JSON file in Google Drive\"\"\"\n",
        "    try:\n",
        "        # Correct path with consistent naming\n",
        "        drive_folder = '/content/drive/MyDrive/ring_tree/ring'\n",
        "        os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "        # Prepare the data to be saved\n",
        "        ring_tree_data = {\n",
        "            'surface': ring_tree.surface,\n",
        "            'surface_rankings': ring_tree.surface_rankings,\n",
        "            'access_counts': ring_tree.access_counts,\n",
        "            'windows': ring_tree.windows,\n",
        "            'branches': ring_tree.branches,\n",
        "            'branch_connections': ring_tree.branch_connections,\n",
        "            'tree_counter': ring_tree.tree_counter,\n",
        "            'branch_counter': ring_tree.branch_counter,\n",
        "            'current_tree': ring_tree.current_tree\n",
        "        }\n",
        "\n",
        "        # Save to a JSON file\n",
        "        file_path = os.path.join(drive_folder, 'ring_tree_state.json')\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(ring_tree_data, f, indent=4)\n",
        "\n",
        "        print(f\"Ring tree state saved to {file_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving ring tree state: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_ring_tree_from_drive():\n",
        "    \"\"\"Load the ring tree structure from a JSON file in Google Drive\"\"\"\n",
        "    try:\n",
        "        # Correct the path\n",
        "        drive_folder = '/content/drive/MyDrive/ring_tree/ring'\n",
        "        file_path = os.path.join(drive_folder, 'ring_tree_state.json')\n",
        "\n",
        "        # Check if the folder exists\n",
        "        if not os.path.exists(drive_folder):\n",
        "            print(f\"Directory not found: {drive_folder}\")\n",
        "            return False\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.isfile(file_path):\n",
        "            print(f\"No existing ring tree state found at {file_path}\")\n",
        "            return False\n",
        "\n",
        "        # Load the JSON file\n",
        "        with open(file_path, 'r') as f:\n",
        "            ring_tree_data = json.load(f)\n",
        "\n",
        "        # Restore the ring tree state\n",
        "        ring_tree.surface = ring_tree_data.get('surface', {})\n",
        "        ring_tree.surface_rankings = ring_tree_data.get('surface_rankings', {\"R'\": {}, \"R''\": {}, \"R'''\" : {}})\n",
        "        ring_tree.access_counts = ring_tree_data.get('access_counts', {})\n",
        "        ring_tree.windows = ring_tree_data.get('windows', {})\n",
        "        ring_tree.branches = ring_tree_data.get('branches', {})\n",
        "        ring_tree.branch_connections = ring_tree_data.get('branch_connections', {})\n",
        "        ring_tree.tree_counter = ring_tree_data.get('tree_counter', 1)\n",
        "        ring_tree.branch_counter = ring_tree_data.get('branch_counter', 1)\n",
        "        ring_tree.current_tree = ring_tree_data.get('current_tree', None)\n",
        "\n",
        "        print(\"Ring tree state loaded successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ring tree state: {e}\")\n",
        "        return False\n",
        "\n",
        "def id_to_list(image_id, original_length):\n",
        "    compressed = base64.urlsafe_b64decode(image_id)\n",
        "    image_bytes = zlib.decompress(compressed)\n",
        "    pixel_list = np.frombuffer(image_bytes, dtype=np.uint8).tolist()\n",
        "    return pixel_list[:original_length]\n",
        "\n",
        "def visualize_processed_pixels(focused_regions, images_per_row=4, max_images_per_figure=32, save_path=\"focused_spots_test.png\"):\n",
        "    \"\"\"\n",
        "    Visualizes processed image regions in a grid format, creating multiple figures if needed.\n",
        "    Each figure contains at most max_images_per_figure images with images_per_row per row.\n",
        "    \"\"\"\n",
        "    num_regions = len(focused_regions)\n",
        "    if num_regions == 0:\n",
        "        print(\"No processed regions to visualize.\")\n",
        "        return\n",
        "\n",
        "    # Calculate number of figures needed\n",
        "    num_figures = (num_regions + max_images_per_figure - 1) // max_images_per_figure\n",
        "\n",
        "    for fig_num in range(num_figures):\n",
        "        # Determine start and end indices for this figure\n",
        "        start_idx = fig_num * max_images_per_figure\n",
        "        end_idx = min((fig_num + 1) * max_images_per_figure, num_regions)\n",
        "        current_batch = focused_regions[start_idx:end_idx]\n",
        "\n",
        "        # Calculate rows needed for this batch\n",
        "        batch_size = len(current_batch)\n",
        "        rows = (batch_size + images_per_row - 1) // images_per_row\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(rows, images_per_row, figsize=(images_per_row * 3, rows * 3))\n",
        "\n",
        "        # Handle single row case\n",
        "        if rows == 1:\n",
        "            axes = np.array([axes])\n",
        "\n",
        "        # Handle single image case\n",
        "        if rows == 1 and images_per_row == 1:\n",
        "            axes = np.array([[axes]])\n",
        "\n",
        "        # Plot images in this batch\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i < batch_size:\n",
        "                reshaped_image = np.array(current_batch[i], dtype=np.uint8).reshape(16, 16)\n",
        "                ax.imshow(reshaped_image, cmap='gray')\n",
        "                ax.set_title(f\"Spot {start_idx + i + 1}\")\n",
        "                ax.axis('off')\n",
        "            else:\n",
        "                ax.axis('off')  # Hide unused subplots\n",
        "\n",
        "        # Set title and save\n",
        "        plt.suptitle(f\"Focused Spots (Batch {fig_num + 1} of {num_figures})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Create batch-specific filename\n",
        "        if num_figures > 1:\n",
        "            base, ext = os.path.splitext(save_path)\n",
        "            batch_save_path = f\"{base}_batch{fig_num + 1}{ext}\"\n",
        "        else:\n",
        "            batch_save_path = save_path\n",
        "\n",
        "        # Save and close figure\n",
        "        plt.savefig(batch_save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Visualization batch {fig_num + 1} saved as {batch_save_path}\")\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "training = False\n",
        "testing = True\n",
        "organizing = False\n",
        "\n",
        "if training == True:\n",
        "    for inputs in load_spot_ids_from_drive_with_waiting():\n",
        "        print(inputs)\n",
        "        print(ring_tree(inputs, training=True))\n",
        "        save_ring_tree_to_drive()\n",
        "\n",
        "elif organizing == True:\n",
        "    for inputs in load_spot_ids_from_drive_with_waiting():\n",
        "        print(ring_tree(inputs, organizing=True))\n",
        "        save_ring_tree_to_drive()\n",
        "else:\n",
        "    count = 0\n",
        "    for inputs in load_spot_ids_from_drive_with_waiting_test():\n",
        "        #print(f\"input - {inputs}\")\n",
        "        #print(f\"input - {type(inputs)}\")\n",
        "        print(ring_tree(inputs, testing=True))\n",
        "        count += 1  # Increment counter\n",
        "        print(f\"count - {count}\")\n",
        "        if count >= 10:  # Stop after 10 iterations\n",
        "            break\n",
        "    visualize_processed_pixels(output2)"
      ],
      "metadata": {
        "id": "omkd_SYRLtfk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}